{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy to Triton Inference Server locally\n",
    "\n",
    "description: (preview) deploy an image classification model trained on densenet locally via Triton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that this Public Preview release is subject to the [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: nvidia-pyindex in /home/gopalv/miniconda3/envs/azureml/lib/python3.7/site-packages (1.0.5)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already up-to-date: tritonclient in /home/gopalv/miniconda3/envs/azureml/lib/python3.7/site-packages (2.4.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.19.1 in /home/gopalv/miniconda3/envs/azureml/lib/python3.7/site-packages (from tritonclient) (1.19.4)\n",
      "Requirement already satisfied, skipping upgrade: python-rapidjson>=0.9.1 in /home/gopalv/miniconda3/envs/azureml/lib/python3.7/site-packages (from tritonclient) (0.9.4)\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: azureml-core==1.19.0a1 from file:///home/gopalv/azureml-examples/tutorials/deploy-triton/azureml_core-1.19.0a1-py3-none-any.whl in /home/gopalv/miniconda3/envs/azureml/lib/python3.7/site-packages (1.19.0a1)\n",
      "Requirement already satisfied: contextlib2 in /home/gopalv/miniconda3/envs/azureml/lib/python3.7/site-packages (from azureml-core==1.19.0a1) (0.6.0.post1)\n",
      "Requirement already satisfied: azure-mgmt-authorization<1.0.0,>=0.40.0 in /home/gopalv/miniconda3/envs/azureml/lib/python3.7/site-packages (from azureml-core==1.19.0a1) (0.60.0)\n",
      "Requirement already satisfied: pytz in /home/gopalv/miniconda3/envs/azureml/lib/python3.7/site-packages (from azureml-core==1.19.0a1) (2020.1)\n",
      "Requirement already satisfied: azure-mgmt-keyvault<7.0.0,>=0.40.0 in /home/gopalv/miniconda3/envs/azureml/lib/python3.7/site-packages (from azureml-core==1.19.0a1) (2.2.0)\n",
      "Requirement already satisfied: pyopenssl<20.0.0 in /home/gopalv/miniconda3/envs/azureml/lib/python3.7/site-packages (from azureml-core==1.19.0a1) (19.1.0)\n",
      "Requirement already satisfied: SecretStorage in /home/gopalv/miniconda3/envs/azureml/lib/python3.7/site-packages (from azureml-core==1.19.0a1) (3.1.2)\n",
      "Requirement already satisfied: PyJWT<2.0.0 in /home/gopalv/miniconda3/envs/azureml/lib/python3.7/site-packages (from azureml-core==1.19.0a1) (1.7.1)\n",
      "Requirement already satisfied: msrestazure>=0.4.33 in /home/gopalv/miniconda3/envs/azureml/lib/python3.7/site-packages (from azureml-core==1.19.0a1) (0.6.4)\n",
      "Requirement already satisfied: azure-graphrbac<1.0.0,>=0.40.0 in /home/gopalv/miniconda3/envs/azureml/lib/python3.7/site-packages (from azureml-core==1.19.0a1) (0.61.1)\n",
      "Requirement already satisfied: azure-mgmt-resource<15.0.0,>=1.2.1 in /home/gopalv/miniconda3/envs/azureml/lib/python3.7/site-packages (from azureml-core==1.19.0a1) (10.1.0)\n",
      "Requirement already satisfied: msrest>=0.5.1 in /home/gopalv/miniconda3/envs/azureml/lib/python3.7/site-packages (from azureml-core==1.19.0a1) (0.6.18)\n",
      "Requirement already satisfied: urllib3>=1.23 in /home/gopalv/miniconda3/envs/azureml/lib/python3.7/site-packages (from azureml-core==1.19.0a1) (1.25.10)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/gopalv/miniconda3/envs/azureml/lib/python3.7/site-packages (from azureml-core==1.19.0a1) (2.8.1)\n",
      "Requirement already satisfied: ndg-httpsclient in /home/gopalv/miniconda3/envs/azureml/lib/python3.7/site-packages (from azureml-core==1.19.0a1) (0.5.1)\n",
      "Requirement already satisfied: jsonpickle in /home/gopalv/miniconda3/envs/azureml/lib/python3.7/site-packages (from azureml-core==1.19.0a1) (1.4.1)\n",
      "Requirement already satisfied: docker in /home/gopalv/miniconda3/envs/azureml/lib/python3.7/site-packages (from azureml-core==1.19.0a1) (4.2.2)\n",
      "Requirement already satisfied: requests>=2.19.1 in /home/gopalv/miniconda3/envs/azureml/lib/python3.7/site-packages (from azureml-core==1.19.0a1) (2.24.0)\n",
      "Requirement already satisfied: backports.tempfile in /home/gopalv/miniconda3/envs/azureml/lib/python3.7/site-packages (from azureml-core==1.19.0a1) (1.0)\n",
      "Requirement already satisfied: ruamel.yaml>=0.15.35 in /home/gopalv/miniconda3/envs/azureml/lib/python3.7/site-packages (from azureml-core==1.19.0a1) (0.16.10)\n",
      "Requirement already satisfied: adal>=1.2.0 in /home/gopalv/miniconda3/envs/azureml/lib/python3.7/site-packages (from azureml-core==1.19.0a1) (1.2.4)\n",
      "Requirement already satisfied: azure-mgmt-containerregistry>=2.0.0 in /home/gopalv/miniconda3/envs/azureml/lib/python3.7/site-packages (from azureml-core==1.19.0a1) (2.8.0)\n",
      "Requirement already satisfied: cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.* in /home/gopalv/miniconda3/envs/azureml/lib/python3.7/site-packages (from azureml-core==1.19.0a1) (3.0)\n",
      "Requirement already satisfied: azure-common>=1.1.12 in /home/gopalv/miniconda3/envs/azureml/lib/python3.7/site-packages (from azureml-core==1.19.0a1) (1.1.25)\n",
      "Requirement already satisfied: azure-mgmt-storage<16.0.0,>=1.5.0 in /home/gopalv/miniconda3/envs/azureml/lib/python3.7/site-packages (from azureml-core==1.19.0a1) (11.1.0)\n",
      "Requirement already satisfied: jmespath in /home/gopalv/miniconda3/envs/azureml/lib/python3.7/site-packages (from azureml-core==1.19.0a1) (0.10.0)\n",
      "Requirement already satisfied: pathspec in /home/gopalv/miniconda3/envs/azureml/lib/python3.7/site-packages (from azureml-core==1.19.0a1) (0.8.0)\n",
      "Requirement already satisfied: six>=1.5.2 in /home/gopalv/miniconda3/envs/azureml/lib/python3.7/site-packages (from pyopenssl<20.0.0->azureml-core==1.19.0a1) (1.15.0)\n",
      "Requirement already satisfied: jeepney>=0.4.2 in /home/gopalv/miniconda3/envs/azureml/lib/python3.7/site-packages (from SecretStorage->azureml-core==1.19.0a1) (0.4.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/gopalv/miniconda3/envs/azureml/lib/python3.7/site-packages (from msrest>=0.5.1->azureml-core==1.19.0a1) (2020.6.20)\n",
      "Requirement already satisfied: requests-oauthlib>=0.5.0 in /home/gopalv/miniconda3/envs/azureml/lib/python3.7/site-packages (from msrest>=0.5.1->azureml-core==1.19.0a1) (1.3.0)\n",
      "Requirement already satisfied: isodate>=0.6.0 in /home/gopalv/miniconda3/envs/azureml/lib/python3.7/site-packages (from msrest>=0.5.1->azureml-core==1.19.0a1) (0.6.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.1 in /home/gopalv/miniconda3/envs/azureml/lib/python3.7/site-packages (from ndg-httpsclient->azureml-core==1.19.0a1) (0.4.8)\n",
      "Requirement already satisfied: importlib-metadata in /home/gopalv/miniconda3/envs/azureml/lib/python3.7/site-packages (from jsonpickle->azureml-core==1.19.0a1) (1.7.0)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /home/gopalv/miniconda3/envs/azureml/lib/python3.7/site-packages (from docker->azureml-core==1.19.0a1) (0.57.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/gopalv/miniconda3/envs/azureml/lib/python3.7/site-packages (from requests>=2.19.1->azureml-core==1.19.0a1) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/gopalv/miniconda3/envs/azureml/lib/python3.7/site-packages (from requests>=2.19.1->azureml-core==1.19.0a1) (3.0.4)\n",
      "Requirement already satisfied: backports.weakref in /home/gopalv/miniconda3/envs/azureml/lib/python3.7/site-packages (from backports.tempfile->azureml-core==1.19.0a1) (1.0.post1)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.1.2; platform_python_implementation == \"CPython\" and python_version < \"3.9\" in /home/gopalv/miniconda3/envs/azureml/lib/python3.7/site-packages (from ruamel.yaml>=0.15.35->azureml-core==1.19.0a1) (0.2.0)\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /home/gopalv/miniconda3/envs/azureml/lib/python3.7/site-packages (from cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.*->azureml-core==1.19.0a1) (1.14.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/gopalv/miniconda3/envs/azureml/lib/python3.7/site-packages (from requests-oauthlib>=0.5.0->msrest>=0.5.1->azureml-core==1.19.0a1) (3.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/gopalv/miniconda3/envs/azureml/lib/python3.7/site-packages (from importlib-metadata->jsonpickle->azureml-core==1.19.0a1) (3.3.0)\n",
      "Requirement already satisfied: pycparser in /home/gopalv/miniconda3/envs/azureml/lib/python3.7/site-packages (from cffi!=1.11.3,>=1.8->cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.*->azureml-core==1.19.0a1) (2.20)\n"
     ]
    }
   ],
   "source": [
    "!pip install nvidia-pyindex\n",
    "!pip install --upgrade tritonclient\n",
    "!curl -L https://aka.ms/azureml-core-1.19.a1 --output azureml_core-1.19.0a1-py3-none-any.whl\n",
    "!pip install azureml_core-1.19.0a1-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Workspace.create(name='Inference-PM-AML-Workspace', subscription_id='92c76a2f-0e1c-4216-b65e-abf7a3f34c1e', resource_group='Inference-PM')"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download model\n",
    "\n",
    "It's important that your model have this directory structure for Triton Inference Server to be able to load it. [Read more about the directory structure that Triton expects](https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/model_repository.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "successfully downloaded model: densenet_onnx\n",
      "successfully downloaded model: bidaf-9\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from src.model_utils import download_triton_models, delete_triton_models\n",
    "\n",
    "prefix = Path(\".\")\n",
    "download_triton_models(prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register model\n",
    "\n",
    "A registered model is a logical container stored in the cloud, containing all files located at `model_path`, which is associated with a version number and other metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Registering model densenet-onnx-example\n",
      "Model(workspace=Workspace.create(name='Inference-PM-AML-Workspace', subscription_id='92c76a2f-0e1c-4216-b65e-abf7a3f34c1e', resource_group='Inference-PM'), name=densenet-onnx-example, id=densenet-onnx-example:9, version=9, tags={'area': 'Image classification', 'type': 'classification'}, properties={})\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.model import Model\n",
    "\n",
    "model_path = \"models\"\n",
    "\n",
    "model = Model.register(\n",
    "    model_path=model_path,\n",
    "    model_name=\"densenet-onnx-example\",\n",
    "    tags={\"area\": \"Image classification\", \"type\": \"classification\"},\n",
    "    description=\"Image classification trained on Imagenet Dataset\",\n",
    "    workspace=ws,\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy webservice\n",
    "\n",
    "In this case we deploy to the local compute, but for other options, see [our documentation](https://docs.microsoft.com/azure/machine-learning/how-to-deploy-and-where?tabs=azcli).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Warning, custom base image or base dockerfile detected without a specified `inferencing_stack_version`. Please set environment.inferencing_stack_version='latest'\n",
      "Downloading model densenet-onnx-example:9 to /tmp/azureml_qohpltm8/densenet-onnx-example/9\n",
      "Generating Docker build context.\n",
      "Package creation Succeeded\n",
      "Logging into Docker registry inferencepmaef584480.azurecr.io\n",
      "Logging into Docker registry inferencepmaef584480.azurecr.io\n",
      "Building Docker image from Dockerfile...\n",
      "Step 1/5 : FROM inferencepmaef584480.azurecr.io/azureml/azureml_0d59db2c4b282d80ea50460afb86a2ee\n",
      " ---> 8ada067971f0\n",
      "Step 2/5 : COPY azureml-app /var/azureml-app\n",
      " ---> 785d7c027f7b\n",
      "Step 3/5 : RUN mkdir -p '/var/azureml-app' && echo eyJhY2NvdW50Q29udGV4dCI6eyJzdWJzY3JpcHRpb25JZCI6IjkyYzc2YTJmLTBlMWMtNDIxNi1iNjVlLWFiZjdhM2YzNGMxZSIsInJlc291cmNlR3JvdXBOYW1lIjoiaW5mZXJlbmNlLXBtIiwiYWNjb3VudE5hbWUiOiJpbmZlcmVuY2UtcG0tYW1sLXdvcmtzcGFjZSIsIndvcmtzcGFjZUlkIjoiODI0NDI5MGEtMzUwNi00MDEyLTgxNjAtOGQ4NjcwMzMzZTE5In0sIm1vZGVscyI6e30sIm1vZGVsc0luZm8iOnt9fQ== | base64 --decode > /var/azureml-app/model_config_map.json\n",
      " ---> Running in 33a0022e8606\n",
      " ---> aa9c06f3ba6f\n",
      "Step 4/5 : RUN mv '/var/azureml-app/tmphfstpm4k.py' /var/azureml-app/main.py\n",
      " ---> Running in 6cc93e87e255\n",
      " ---> 34d0c2288186\n",
      "Step 5/5 : CMD [\"bash\",\"-c\",\"source activate '/opt/miniconda'; export AZUREML_CONDA_ENVIRONMENT_PATH=${AZUREML_CONDA_ENVIRONMENT_PATH:=$CONDA_PREFIX}; exec 'runsvdir' '/var/runit'\"]\n",
      " ---> Running in 7ecce04b1c62\n",
      " ---> e61717e4a2ac\n",
      "Successfully built e61717e4a2ac\n",
      "Successfully tagged triton-densenet-onnx-local23422:latest\n",
      "Starting Docker container...\n",
      "Docker container running.\n",
      "Checking container health...\n",
      "Local webservice is running at http://localhost:6789\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.webservice import LocalWebservice\n",
    "from azureml.core import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.model import InferenceConfig\n",
    "from random import randint\n",
    "\n",
    "service_name = \"triton-densenet-onnx-local\" + str(randint(10000, 99999))\n",
    "\n",
    "\n",
    "env = Environment(\"triton-tutorial\")\n",
    "env.docker.base_image = None\n",
    "env.docker.base_dockerfile = \"Dockerfile\"\n",
    "env.python.user_managed_dependencies = True\n",
    "env.python.interpreter_path = \"/opt/miniconda/bin/python\"\n",
    "\n",
    "env.environment_variables[\"WORKER_COUNT\"] = \"1\"\n",
    "\n",
    "inference_config = InferenceConfig(\n",
    "    # this entry script is where we dispatch a call to the Triton server\n",
    "    source_directory=\"src\",\n",
    "    entry_script=\"score_densenet.py\",\n",
    "    environment=env,\n",
    ")\n",
    "\n",
    "config = LocalWebservice.deploy_configuration(port=6789)\n",
    "\n",
    "service = Model.deploy(\n",
    "    workspace=ws,\n",
    "    name=service_name,\n",
    "    models=[model],\n",
    "    inference_config=inference_config,\n",
    "    deployment_config=config,\n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "service.wait_for_deployment(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2020-11-19T06:50:25,842226200+00:00 - gunicorn/run \n2020-11-19T06:50:25,842401500+00:00 - rsyslog/run \n2020-11-19T06:50:25,842413500+00:00 - triton/run \n\n=============================\n== Triton Inference Server ==\n=============================\n\nNVIDIA Release 20.08 (build 15533555)\n\nCopyright (c) 2018-2020, NVIDIA CORPORATION.  All rights reserved.\n\nVarious files include modifications (c) NVIDIA CORPORATION.  All rights reserved.\nNVIDIA modifications are covered by the license terms that apply to the underlying\nproject or file.\n2020-11-19T06:50:25,869858400+00:00 - Waiting for Triton server to get ready ...\nfind: File system loop detected; ‘/usr/bin/X11’ is part of the same file system loop as ‘/usr/bin’.\n\nWARNING: The NVIDIA Driver was not detected.  GPU functionality will not be available.\n   Use 'nvidia-docker run' to start this container; see\n   https://github.com/NVIDIA/nvidia-docker/wiki/nvidia-docker .\n\nNOTE: The SHMEM allocation limit is set to the default of 64MB.  This may be\n   insufficient for the inference server.  NVIDIA recommends the use of the following flags:\n   nvidia-docker run --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 ...\n\nI1119 06:50:26.924626 27 server.cc:119] Initializing Triton Inference Server\nE1119 06:50:26.924959 27 pinned_memory_manager.cc:192] failed to allocate pinned system memory: CUDA driver version is insufficient for CUDA runtime version\nI1119 06:50:27.282407 27 model_repository_manager.cc:737] loading: bidaf-9:1\nI1119 06:50:27.282619 27 model_repository_manager.cc:737] loading: densenet_onnx:1\nWARNING: Since openmp is enabled in this build, this API cannot be used to configure intra op num threads. Please use the openmp environment variables to control the number of threads.\nWARNING: Since openmp is enabled in this build, this API cannot be used to configure intra op num threads. Please use the openmp environment variables to control the number of threads.\nI1119 06:50:27.297412 27 onnx_backend.cc:195] Creating instance densenet_onnx_0_cpu on CPU using model.onnx\nWARNING: Since openmp is enabled in this build, this API cannot be used to configure intra op num threads. Please use the openmp environment variables to control the number of threads.\nWARNING: Since openmp is enabled in this build, this API cannot be used to configure intra op num threads. Please use the openmp environment variables to control the number of threads.\nI1119 06:50:27.301236 27 onnx_backend.cc:195] Creating instance bidaf-9_0_cpu on CPU using model.onnx\nI1119 06:50:27.679051 27 model_repository_manager.cc:925] successfully loaded 'bidaf-9' version 1\nI1119 06:50:27.682403 27 model_repository_manager.cc:925] successfully loaded 'densenet_onnx' version 1\nI1119 06:50:27.684323 27 grpc_server.cc:3897] Started GRPCInferenceService at 0.0.0.0:8001\nI1119 06:50:27.684641 27 http_server.cc:2679] Started HTTPService at 0.0.0.0:8000\nI1119 06:50:27.727437 27 http_server.cc:2698] Started Metrics Service at 0.0.0.0:8002\n2020-11-19T06:50:30,892755600+00:00 - Triton server ready, starting workers ...\nStarting gunicorn 19.9.0\nListening at: http://0.0.0.0:5001 (26)\nUsing worker: sync\nworker timeout is set to 300\nBooting worker with pid: 115\nSPARK_HOME not set. Skipping PySpark Initialization.\nInitializing logger\n2020-11-19 06:50:36,418 | root | INFO | Starting up app insights client\nStarting up app insights client\n2020-11-19 06:50:36,418 | root | INFO | Starting up request id generator\nStarting up request id generator\n2020-11-19 06:50:36,418 | root | INFO | Starting up app insight hooks\nStarting up app insight hooks\n2020-11-19 06:50:36,418 | root | INFO | Invoking user's init function\nInvoking user's init function\n/opt/miniconda/lib/python3.7/site-packages/tritonhttpclient/__init__.py:33: DeprecationWarning: The package `tritonhttpclient` is deprecated and will be removed in a future version. Please use instead `tritonclient.http`\n  \"`tritonclient.http`\", DeprecationWarning)\n2020-11-19 06:50:36,428 | root | INFO | Users's init has completed successfully\nUsers's init has completed successfully\n2020-11-19 06:50:36,432 | root | INFO | Skipping middleware: dbg_model_info as it's not enabled.\nSkipping middleware: dbg_model_info as it's not enabled.\n2020-11-19 06:50:36,432 | root | INFO | Skipping middleware: dbg_resource_usage as it's not enabled.\nSkipping middleware: dbg_resource_usage as it's not enabled.\n2020-11-19 06:50:36,433 | root | INFO | Scoring timeout setting is not found. Use default timeout: 3600000 ms\nScoring timeout setting is not found. Use default timeout: 3600000 ms\n\n"
     ]
    }
   ],
   "source": [
    "print(service.get_logs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the webservice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "unexpected shape for input 'data_0' for model 'densenet_onnx'. Expected [3,224,224], got [1,3,224,224]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "headers = {\"Content-Type\": \"application/octet-stream\"}\n",
    "\n",
    "test_sample = requests.get(\"https://aka.ms/peacock-pic\", allow_redirects=True).content\n",
    "resp = requests.post(service.scoring_uri, data=test_sample, headers=headers)\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the webservice and the downloaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Container has been successfully cleaned up.\nmodel: densenet_onnx was already deleted\nmodel: bidaf-9 was already deleted\n"
     ]
    }
   ],
   "source": [
    "service.delete()\n",
    "delete_triton_models(prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "\n",
    "Try changing the deployment configuration to [deploy to Azure Kubernetes Service](https://docs.microsoft.com/azure/machine-learning/how-to-deploy-azure-kubernetes-service?tabs=python) for higher availability and better scalability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.7 64-bit ('azureml': conda)",
   "metadata": {
    "interpreter": {
     "hash": "53514593536e52de022f29ef618678eddccd581b6db5dc532e9838fb19203af5"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "name": "deploy-densenet-local",
  "task": "Use the high-performance Triton Inference Server with Azure Machine Learning"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}