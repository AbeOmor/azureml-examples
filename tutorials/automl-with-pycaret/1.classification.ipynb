{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "# Train a binary classification model"
=======
    "# Train a binary classification model\n",
    "\n",
    "In this tutorial, we walk through a simple binary classificaiton problem using PyCaret."
>>>>>>> origin/main
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pycaret scikit-plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "## Get workspace"
=======
    "## Setup cloud tracking\n",
    "\n",
    "[Mlflow](https://github.com/mlflow/mlflow) is a great tool for local ML experimentation tracking. However, using it alone is like using git without GitHub. Your Azure Machine Learning workspace can easily be used to setup a remote tracking URI for mlflow:"
>>>>>>> origin/main
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "ws"
=======
    "import mlflow\n",
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())"
>>>>>>> origin/main
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "## Setup mlflow tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())\n",
    "\n",
    "# azure ml settings\n",
    "experiment_name = \"automl-with-pycaret-tutorial\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a pandas.DataFrame"
=======
    "## Load a pandas.DataFrame\n",
    "\n",
    "The PyCaret datasets module contains many sample datasets. Try replacing with your own data!"
>>>>>>> origin/main
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.datasets import get_data\n",
    "\n",
    "df = get_data(\"credit\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "## Split data"
=======
    "## Split data\n",
    "\n",
    "Split the data into training data (for modeling) and test data (for prediction):"
>>>>>>> origin/main
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.sample(frac=0.95, random_state=42)\n",
    "data_unseen = df.drop(data.index)\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "data_unseen.reset_index(inplace=True, drop=True)\n",
    "print(\"Data for modeling: \" + str(data.shape))\n",
    "print(\"Unseen data for predictions: \" + str(data_unseen.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "## Import PyCaret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
=======
    "## Setup PyCaret\n",
    "\n",
>>>>>>> origin/main
    "The `setup()` function initializes the environment in pycaret and creates the transformation pipeline to prepare the data for modeling and deployment. \n",
    "\n",
    "`setup()` must be called before executing any other function in pycaret. \n",
    "\n",
<<<<<<< HEAD
    "It takes two mandatory parameters: a `pandas.DataFrame` and the name of the target column. All other parameters are optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.classification import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check supported algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup PyCaret\n",
=======
    "It takes two mandatory parameters: a `pandas.DataFrame` and the name of the target column. All other parameters are optional.\n",
>>>>>>> origin/main
    "\n",
    "Refer to the [PyCaret documentation](https://pycaret.readthedocs.io/en/stable/) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
=======
    "from pycaret.classification import *\n",
    "\n",
>>>>>>> origin/main
    "exp = setup(\n",
    "    data=data,\n",
    "    target=\"default\",\n",
    "    log_experiment=True,\n",
<<<<<<< HEAD
    "    experiment_name=experiment_name,\n",
=======
    "    experiment_name=\"automl-with-pycaret-tutorial\",\n",
>>>>>>> origin/main
    "    log_plots=True,\n",
    "    log_profile=True,\n",
    "    silent=True,  # set to False for interactively setting data types\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "## Run AutoML"
=======
    "## Run AutoML\n",
    "\n",
    "Run a series of trials to find the best model."
>>>>>>> origin/main
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "best_model = compare_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "## Evaluate model"
=======
    "## Evaluate model\n",
    "\n",
    "Evaluate the best model."
>>>>>>> origin/main
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "## Test model"
=======
    "## Test model\n",
    "\n",
    "Evaluate the best model on unseen data."
>>>>>>> origin/main
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_predictions = predict_model(best_model, data=data_unseen)\n",
    "unseen_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycaret.utils import check_metric\n",
    "\n",
    "check_metric(\n",
    "    unseen_predictions.default,\n",
    "    unseen_predictions.Label.astype(int),\n",
    "    \"Accuracy\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
