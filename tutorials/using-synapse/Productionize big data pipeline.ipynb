{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure Machine Learning and Synapse integration (Private preview)\n",
    "### Tutorial: Productionize machine learning pipeline by leveraging spark backed by Synapse\n",
    "\n",
    "In this notebook, you learn the following tasks: \n",
    "* Leverage spark pools as compute target for Azure ML pipeline run\n",
    "* Leverage spark pools as compute target for Azure ML experiment run\n",
    "\n",
    "**Contents**:\n",
    "* Package installation\n",
    "* Get compute targets for pipeline run\n",
    "* Submit a two-step pipeline run, including big data processing on spark cluster and training on AML compute instance.\n",
    "* Submit an experiment job to spark pool.\n",
    "\n",
    "**Please refer to \"Run ML flow E2E in notebook\" notebook for first time setup (for example, linking Synapse workspace and attaching Synapse Spark pools to Azure ML)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step1: Install/update Azure ML packages\n",
    "\n",
    "Note: In private preview, please install the packages below. In public preview, this step will be removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U \"azureml-core<0.1.10\" --index-url https://azuremlsdktestpypi.azureedge.net/SynapseInAml/ --extra-index-url https://pypi.python.org/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U \"azureml-pipeline-core<0.1.10\" --index-url https://azuremlsdktestpypi.azureedge.net/SynapseInAml/ --extra-index-url https://pypi.python.org/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U \"azureml-pipeline-steps<0.1.10\" --index-url https://azuremlsdktestpypi.azureedge.net/SynapseInAml/ --extra-index-url https://pypi.python.org/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please restart kernal once installation completes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step2: Get attached spark pool and compute instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime  \n",
    "from azureml.core import Workspace, Experiment, Dataset, Environment,Datastore, LinkedWorkspace\n",
    "\n",
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# retrieve attached Synapse spark pool\n",
    "synapse_compute = ws.compute_targets['<Synapse Spark pool alias in AML>']\n",
    "synapse_compute\n",
    "\n",
    "# Choose a name for your CPU cluster\n",
    "cpu_cluster_name = \"cpucluster\"\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2',\n",
    "                                                           max_nodes=1)\n",
    "    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
    "\n",
    "cpu_cluster.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step3: Prepare for pipeline input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "from azureml.data.dataset_factory import DataType\n",
    "\n",
    "dataset_name=\"blob_ds\"\n",
    "try:\n",
    "    dataset = Dataset.get_by_name(workspace=ws, name=dataset_name)\n",
    "    print('Found existing dataset, use it.')\n",
    "except:\n",
    "    # create a TabularDataset from a delimited file behind a public web url and convert column \"Survived\" to boolean\n",
    "    web_path ='https://dprepdata.blob.core.windows.net/demo/Titanic.csv'\n",
    "    titanic_ds = Dataset.Tabular.from_delimited_files(path=web_path, set_column_types={'Survived': DataType.to_bool()})\n",
    "    titanic_ds.register(ws,name=dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step4: Start a pipeline run \n",
    "View run logs and output in Azure Machine Learning Studio run detail page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep, SynapseSparkStep\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# Add training config including dependencies\n",
    "train_run_config = RunConfiguration()\n",
    "conda = CondaDependencies.create(\n",
    "    pip_indexurl='https://azuremlsdktestpypi.azureedge.net/sdk-release/master/588E708E0DF342C4A80BD954289657CF',\n",
    "    pip_packages=['azureml-sdk<0.1.1', 'azureml-dataprep[fuse,pandas]>=1.1.19', 'azureml-telemetry'],\n",
    "    pin_sdk_version=False\n",
    ")\n",
    "\n",
    "\n",
    "conda.set_pip_option('--pre')\n",
    "train_run_config.environment.python.conda_dependencies = conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.data import HDFSOutputDatasetConfig\n",
    "\n",
    "ds = Dataset.get_by_name(ws,name='blob_ds')\n",
    "input1 = ds.as_named_input('synapseinput')\n",
    "\n",
    "output1 = HDFSOutputDatasetConfig(\n",
    "    \"synapse_step_output\", destination=(ws.datastores['<datastore name>'],\"<folder name>\")).register_on_complete(name=\"<registered dataset name>\")\n",
    "\n",
    "input2 = output1.as_input(\"input2\").as_download()\n",
    "\n",
    "\n",
    "step_1 = SynapseSparkStep(name = 'synapse-spark',\n",
    "                          file = 'pyspark_job_pipeline.py',\n",
    "                          source_directory=\".\", \n",
    "                          #arguments=[myinput, output1],\n",
    "                          inputs=[input1],\n",
    "                          outputs=[output1],\n",
    "                          compute_target = synapse_compute,\n",
    "                          driver_memory = \"7g\",\n",
    "                          driver_cores = 4,\n",
    "                          executor_memory = \"7g\",\n",
    "                          executor_cores = 2,\n",
    "                          num_executors = 1)\n",
    "\n",
    "step_2 = PythonScriptStep(script_name=\"train.py\",\n",
    "                          arguments=[input2],\n",
    "                          inputs=[input2],\n",
    "                          #outputs=[output2],\n",
    "                          compute_target=cpu_cluster_name,\n",
    "                          runconfig = train_run_config,\n",
    "                          source_directory=\".\",\n",
    "                          allow_reuse=False)\n",
    "\n",
    "pipeline = Pipeline(workspace=ws, steps=[step_1, step_2])\n",
    "pipeline_run = pipeline.submit('two_steps', regenerate_outputs=True)\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other option: start an experient run\n",
    "View run logs and output in Azure Machine Learning Studio run detail page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import RunConfiguration\n",
    "from azureml.data import HDFSOutputDatasetConfig\n",
    "output = HDFSOutputDatasetConfig(\n",
    "    \"synapse_step_output\",\n",
    "    destination=(ws.datastores['<datastore name>'],\"<folder name>\")).register_on_complete(name=\"<registered dataset name\")\n",
    "\n",
    "run_config = RunConfiguration(framework=\"pyspark\")\n",
    "run_config.output_data = {output.name: output}\n",
    "\n",
    "run_config.target = '<Spark pool alias in AML>'\n",
    "\n",
    "run_config.spark.configuration[\"spark.driver.memory\"] = \"1g\" \n",
    "run_config.spark.configuration[\"spark.driver.cores\"] = 2 \n",
    "run_config.spark.configuration[\"spark.executor.memory\"] = \"1g\" \n",
    "run_config.spark.configuration[\"spark.executor.cores\"] = 1 \n",
    "run_config.spark.configuration[\"spark.executor.instances\"] = 1 \n",
    "\n",
    "from azureml.core import ScriptRunConfig \n",
    "\n",
    "script_run_config = ScriptRunConfig(source_directory = '.', \n",
    "                                    script= 'pyspark_job_exp.py', \n",
    "                                    arguments = ['args1','args2'], \n",
    "                                    run_config = run_config) \n",
    "\n",
    "from azureml.core import Experiment \n",
    "exp = Experiment(workspace=ws, name=\"synapse-spark\") \n",
    "run = exp.submit(config=script_run_config) \n",
    "run"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
