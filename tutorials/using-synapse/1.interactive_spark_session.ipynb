{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Spark Session on Synapse Compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install package (This will be updated after the offical package release)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt install libkrb5-dev -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U azureml-synapse --index-url https://azuremlsdktestpypi.azureedge.net/sdk-release/master/588E708E0DF342C4A80BD954289657CF --extra-index-url https://pypi.python.org/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For JupyterLab, please additionally run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter lab build --minimize=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__PLEASE restart kernel and then refresh web page before starting spark session.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Magic Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-05T03:22:14.965395Z",
     "iopub.status.busy": "2020-06-05T03:22:14.965395Z",
     "iopub.status.idle": "2020-06-05T03:22:14.970398Z",
     "shell.execute_reply": "2020-06-05T03:22:14.969397Z",
     "shell.execute_reply.started": "2020-06-05T03:22:14.965395Z"
    }
   },
   "outputs": [],
   "source": [
    "# show help\n",
    "%synapse ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Start Synapse Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Synapse compute linked to the NBVM's workspace.\n",
    "\n",
    "%synapse start -c synapsecompute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Synapse compute from anther workspace via its config file\n",
    "\n",
    "# %synapse start -c synapse_compute_name -f config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use Synapse compute from anther workspace via subscription_id, resource_group and workspace_name\n",
    "\n",
    "# %synapse start -c synapse_compute_name -s 35f16a99-532a-4a47-9e93-00305f6c40f2 -r data4mlccanary -w data4mlccanary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data prepration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example-1: Read data by OpenDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%synapse\n",
    "\n",
    "# 10 hottest stations in April 2020\n",
    "\n",
    "from azureml.opendatasets import NoaaIsdWeather\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from  pyspark.sql.functions import desc\n",
    "\n",
    "start_date = parser.parse('2020-4-1')\n",
    "end_date = parser.parse('2020-4-30')\n",
    "isd = NoaaIsdWeather(start_date, end_date)\n",
    "df = isd.to_spark_dataframe()\n",
    "\n",
    "df.groupBy('stationName','countryOrRegion').max('temperature').orderBy(desc('max(temperature)')).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example-2: Read data by HDFS path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-04T08:11:18.812276Z",
     "iopub.status.busy": "2020-06-04T08:11:18.812276Z",
     "iopub.status.idle": "2020-06-04T08:11:23.854526Z",
     "shell.execute_reply": "2020-06-04T08:11:23.853525Z",
     "shell.execute_reply.started": "2020-06-04T08:11:18.812276Z"
    }
   },
   "outputs": [],
   "source": [
    "%%synapse\n",
    "\n",
    "from pyspark.sql.functions import col, desc\n",
    "\n",
    "df = spark.read.option(\"header\", \"true\").csv(\"wasbs://demo@dprepdata.blob.core.windows.net/Titanic.csv\")\n",
    "df.filter(col('Survived') == 1).groupBy('Age').count().orderBy(desc('count')).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Session Metadata\n",
    "After session started, you can check the session's metadata, find the links to Synapse portal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%synapse meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Stop Session\n",
    "When current session reach the status timeout, dead or any failure, you must explicitly stop it before start new one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%synapse stop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}