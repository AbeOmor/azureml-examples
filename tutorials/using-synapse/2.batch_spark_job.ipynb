{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Spark Job on Synapse Compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare your AML workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "ws"
   ]
  },
  {
   "source": [
    "## Input data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "from azureml.data.dataset_factory import DataType\n",
    "\n",
    "dataset_name=\"blob_ds\"\n",
    "try:\n",
    "    dataset = Dataset.get_by_name(workspace=ws, name=dataset_name)\n",
    "except:\n",
    "    # create a TabularDataset from a delimited file behind a public web url and convert column \"Survived\" to boolean\n",
    "    web_path ='https://dprepdata.blob.core.windows.net/demo/Titanic.csv'\n",
    "    titanic_ds = Dataset.Tabular.from_delimited_files(path=web_path, set_column_types={'Survived': DataType.to_bool()})\n",
    "    titanic_ds.register(ws,name=dataset_name)\n",
    "    dataset = Dataset.get_by_name(workspace=ws, name=dataset_name)\n",
    "\n",
    "input = dataset.as_named_input(\"synapse_input\")"
   ]
  },
  {
   "source": [
    "## Output Config"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.data import HDFSOutputDatasetConfig\n",
    "output = HDFSOutputDatasetConfig(\"synapse_output\", (ws.datastores['workspaceblobstore'],\"test\"))"
   ]
  },
  {
   "source": [
    "## dataprep script"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"code\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code/test.py\n",
    "import os\n",
    "import azureml.core\n",
    "from azureml.core import Workspace, Run\n",
    "\n",
    "print(azureml.core.VERSION)\n",
    "print(os.environ['synapse_input'])\n",
    "print(os.environ['synapse_output'])\n",
    "\n",
    "run_context = Run.get_context()\n",
    "dataset = run_context.input_datasets['synapse_input']\n",
    "sdf = dataset.to_spark_dataframe()\n",
    "sdf.show()\n",
    "\n",
    "sdf.coalesce(1).write\\\n",
    ".option(\"header\", \"true\")\\\n",
    ".csv(os.environ['synapse_output'],mode='overwrite')"
   ]
  },
  {
   "source": [
    "## Submit an Experiment "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import RunConfiguration\n",
    "from azureml.core import ScriptRunConfig \n",
    "from azureml.core import Experiment \n",
    "\n",
    "run_config = RunConfiguration(framework=\"pyspark\")\n",
    "run_config.target = 'synapsecompute'\n",
    "\n",
    "run_config.spark.configuration[\"spark.driver.memory\"] = \"1g\" \n",
    "run_config.spark.configuration[\"spark.driver.cores\"] = 2 \n",
    "run_config.spark.configuration[\"spark.executor.memory\"] = \"1g\" \n",
    "run_config.spark.configuration[\"spark.executor.cores\"] = 1 \n",
    "run_config.spark.configuration[\"spark.executor.instances\"] = 1 \n",
    "\n",
    "\n",
    "script_run_config = ScriptRunConfig(source_directory = './code',\n",
    "                                    script= 'test.py',\n",
    "                                    arguments = [input,output],\n",
    "                                    run_config = run_config) \n",
    "\n",
    "\n",
    "exp = Experiment(workspace=ws, name=\"synapse-spark\") \n",
    "run = exp.submit(config=script_run_config) \n",
    "run"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (test)",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}