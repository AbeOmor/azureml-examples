{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "# Introduction to Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade dask distributed bokeh adlfs fsspec fastparquet pyarrow python-snappy lz4"
=======
    "# Introduction to Dask\n",
    "\n",
    "In this notebook, we'll learn how to use [Dask](https://dask.org) for reading data from Azure.\n",
    "\n",
    "The main [dask](https://github.com/dask/dask) and [distributed](https://github.com/dask/distributed) themselves are small and focused. Thousands of tools, some built by the Dask organization and most not, utilize Dask for parallel or distributed processing. Some of the most useful for data science include:\n",
    "\n",
    "- https://github.com/dask/dask-ml\n",
    "- https://github.com/dask/dask-kubernetes\n",
    "- https://github.com/dask/dask-cloudprovider\n",
    "- https://github.com/dask/dask-mpi\n",
    "- https://github.com/dask/adlfs\n",
    "- https://github.com/pydata/xarray\n",
    "- https://github.com/dmlc/xgboost\n",
    "- https://github.com/dask/dask-lightgbm\n",
    "- https://github.com/rapidsai/cudf\n",
    "- https://github.com/rapidsai/cuml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install required packages"
>>>>>>> origin/main
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import git\n",
    "from pathlib import Path\n",
    "\n",
    "# get root of git repo\n",
    "prefix = Path(git.Repo(\".\", search_parent_directories=True).working_tree_dir)\n",
    "\n",
    "# setup data path\n",
    "data_path = prefix.joinpath(\"data\", \"raw\", \"iris\", \"iris.csv\")"
=======
    "!pip install --upgrade dask distributed bokeh adlfs fsspec fastparquet pyarrow python-snappy lz4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get AML Workspace\n",
    "\n",
    "You can use the AML workspace to retrieve datastores and keyvaults for accessing data credentials securely."
>>>>>>> origin/main
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "from dask.distributed import Client\n",
    "\n",
    "c = Client()\n",
    "c"
=======
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "ws"
>>>>>>> origin/main
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "## Pandas and Dask on local data"
=======
    "## Create a distributed client\n",
    "\n",
    "The [client](https://distributed.dask.org/en/latest/client.html) is the primary entrypoint for parallel processing with Dask. Calling it without inputs will create a local distributed scheduler, utilizing all the CPUs and cores on your machine. This can be useful for faster processing of larger in memory dataframes, or even computations on out of memory (OOM) data. \n",
    "\n",
    "When your local machine isn't powerful enough, you can provision a larger VM in Azure - the M series has 100+ CPUs and TBs of RAM. If this still isn't powerful enough, you can create a distributed Dask cluster on most hardware - see [the Dask setup guide](https://docs.dask.org/en/latest/setup.html) for details.\n",
    "\n",
    "If you still need acceleration, [RAPIDSAI](https://github.com/rapidsai) further extends the PyData APIs on GPUs.\n",
    "\n",
    "**Make sure you check out the dashboard!**"
>>>>>>> origin/main
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "import pandas as pd\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = pd.read_csv(data_path)"
=======
    "from distributed import Client\n",
    "\n",
    "c = Client()\n",
    "c.restart()\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading cloud data\n",
    "\n",
    "Reading data from the cloud is as easy as reading it locally! Sorta!\n",
    "\n",
    "### Pandas\n",
    "\n",
    "You can read directly into Pandas from most cloud storage, with a notable exception - from the [`pandas.read_csv` documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html?highlight=read_csv#pandas-read-csv):\n",
    "\n",
    "> Any valid string path is acceptable. The string could be a URL. Valid URL schemes include http, https, ftp, s3, gs, and file.\n",
    "\n",
    "### Pythonic Filesystem for Azure\n",
    "\n",
    "Fortunately, similar protocols have been developed for Azure storage in the [ADLFS](https://github.com/dask/adlfs) package, including:\n",
    "\n",
    "- `az` or `abfs` for Azure Data Lake Storage Gen2 (ADLSv2) and Blob\n",
    "- `adl` for Azure Data Lake Storage Gen1 (ADLSv1)\n",
    "\n",
    "These are included in Python's `fsspec`. You can use the protocol directly in Dask and convert to Pandas for now.\n"
>>>>>>> origin/main
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "%%time\n",
    "ddf = dd.read_csv(data_path)"
=======
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "\n",
    "from adlfs import AzureBlobFileSystem"
>>>>>>> origin/main
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "%%time\n",
    "df.describe()"
=======
    "# for single files in public blobs, you can use the https protocol and read directly into Pandas\n",
    "df = pd.read_csv(\n",
    "    \"https://azuremlexamples.blob.core.windows.net/datasets/iris.csv\"\n",
    ")\n",
    "df.head()"
>>>>>>> origin/main
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "%%time\n",
    "ddf.describe().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data from Azure"
=======
    "# the same with Dask\n",
    "df = dd.read_csv(\n",
    "    \"https://azuremlexamples.blob.core.windows.net/datasets/iris.csv\"\n",
    ")\n",
    "df.head()"
>>>>>>> origin/main
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "ds = ws.get_default_datastore()\n",
    "ds.upload_files([str(data_path)], target_path=\"datasets/iris\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container_name = ds.container_name\n",
=======
    "# alternative syntax in Dask\n",
    "storage_options = {\"account_name\": \"azuremlexamples\"}\n",
    "df = dd.read_csv(f\"az://datasets/iris.csv\", storage_options=storage_options)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why use Dask?\n",
    "\n",
    "Those all did the same thing...so why use Dask? There are a few scenarios:\n",
    "\n",
    "- reading multiple files \n",
    "- reading private data from Azure with credentials\n",
    "- reading directly into GPUs (with [cuDF](https://github.com/rapidsai/cudf))\n",
    "\n",
    "You can also use the classes implemented in `adlfs` to query for files, depending on permissions.\n",
    "\n",
    "To provide your own credentials, refer to the `adlfs` documentation for details - generally you can retrieve credentials from the workspace's datastore:\n",
    "\n",
    "```python\n",
    "import dask.dataframe as dd\n",
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "ds = ws.get_default_datastore() # ws.datastores[\"my-datastore-name\"]\n",
>>>>>>> origin/main
    "\n",
    "storage_options = {\n",
    "    \"account_name\": ds.account_name,\n",
    "    \"account_key\": ds.account_key,\n",
<<<<<<< HEAD
    "}"
=======
    "}\n",
    "\n",
    "df = dd.read_parquet(f\"az://{ds.container_name}/path/to/data/*.parquet\", storage_options=storage_options)\n",
    "```\n",
    "\n",
    "The basics are demonstrated below on public data."
>>>>>>> origin/main
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "%%time\n",
    "ddf = dd.read_csv(\n",
    "    f\"az://{container_name}/datasets/iris/iris.csv\",\n",
    "    storage_options=storage_options,\n",
    ")"
=======
    "color = \"green\"\n",
    "container_name = \"nyctlc\"\n",
    "storage_options = {\"account_name\": \"azureopendatastorage\"}"
>>>>>>> origin/main
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "%%time\n",
    "ddf.describe().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to Pandas"
=======
    "fs = AzureBlobFileSystem(**storage_options)\n",
    "fs"
>>>>>>> origin/main
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "%%time\n",
    "df = ddf.compute()"
=======
    "fs.ls(f\"{container_name}\")"
>>>>>>> origin/main
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "%%time\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigger data"
=======
    "fs.ls(f\"{container_name}/{color}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.ls(f\"{container_name}/{color}/puYear=2016/\")"
>>>>>>> origin/main
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "container_name = \"isdweatherdatacontainer\"\n",
    "\n",
    "storage_options = {\"account_name\": \"azureopendatastorage\"}"
=======
    "files = fs.glob(f\"{container_name}/{color}/puYear=2016/puMonth=12/*.parquet\")\n",
    "files = [f\"az://{file}\" for file in files]\n",
    "files[-5:]"
>>>>>>> origin/main
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "metadata": {},
   "outputs": [],
   "source": [
    "from adlfs import AzureBlobFileSystem\n",
    "\n",
    "fs = AzureBlobFileSystem(**storage_options)\n",
    "fs"
=======
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "ddf = (\n",
    "    dd.read_parquet(files, storage_options=storage_options)\n",
    "    .repartition(npartitions=8)\n",
    "    .persist()\n",
    ")\n",
    "ddf"
>>>>>>> origin/main
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
<<<<<<< HEAD
    "fs.ls(f\"{container_name}/ISDWeather/year=2020\")"
=======
    "len(ddf)"
>>>>>>> origin/main
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "files = fs.glob(f\"{container_name}/ISDWeather/year=2020/month=2/*.parquet\")\n",
    "files = [f\"az://{file}\" for file in files]\n",
    "files[-5:]"
=======
    "%%time\n",
    "len(ddf)"
>>>>>>> origin/main
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "%%time\n",
    "ddf = dd.read_parquet(files, storage_options=storage_options, chunksize=\"20MB\")\n",
    "ddf"
=======
    "ddf.info()"
>>>>>>> origin/main
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "ddf = ddf.persist()"
=======
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(\"dark_background\")\n",
    "\n",
    "ddf[\"tipAmount\"].compute().hist(\n",
    "    figsize=(16, 8), bins=256, range=(0.1, 20),\n",
    ")"
>>>>>>> origin/main
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "%%time\n",
    "len(ddf)"
=======
    "df = ddf.compute()\n",
    "df.info()"
>>>>>>> origin/main
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
<<<<<<< HEAD
    "len(ddf)"
=======
    "df.describe()"
>>>>>>> origin/main
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
<<<<<<< HEAD
    "ddf.describe().compute()"
=======
    "gbs = round(df.memory_usage(index=True, deep=True).sum() / 1e9, 2)\n",
    "print(f\"df is {gbs} GBs\")"
>>>>>>> origin/main
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gbs = round(ddf.memory_usage(index=True, deep=True).sum().compute() / 1e9, 2)\n",
    "print(f\"ddf is {gbs} GBs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
